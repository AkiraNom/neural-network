{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "392088fd-80db-471c-88a9-8694d4b9073d",
   "metadata": {},
   "source": [
    "### Historical CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59d6c66-6043-4c3c-83d3-91c3395b1419",
   "metadata": {},
   "source": [
    "### LeNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "788151b7-b532-4016-8861-a601e6dad976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from torchvision.models.vgg import VGG11_Weights\n",
    "\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ee15ae3-5fb6-4e07-8579-4973d9f2b0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,),std=(0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data/mnist_data', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d4fdad-f35a-408b-a86f-ed0d6e1125dd",
   "metadata": {},
   "source": [
    "<img src=\"./rsc/LeNet.png\" width=\"600\" height=\"800\">\n",
    "\n",
    "LeNet architecture as published in [original paper](https://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf) by LeCun et al. 1998.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21fad7d0-f610-4f0f-8b8c-dceedade1c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1,padding=0)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1,padding=0)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(16*5*5,120)\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = F.max_pool2d(F.relu(self.conv1(X)),2)\n",
    "        X = F.max_pool2d(F.relu(self.conv2(X)),2)\n",
    "        X = self.flatten(X)\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        X = F.relu(self.fc3(X))\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a157e067-6a1a-4ea7-8cb8-8a52a54bf6c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenet = LeNet()\n",
    "lenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0cc7819-70d5-4bcd-9c1f-75208903c5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= torch.randn((128,1,32,32))\n",
    "out = lenet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2ca3013-d385-4bce-ba00-0523254b0803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 10])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17159152-1803-4d1f-b9cf-3cd39bb4124e",
   "metadata": {},
   "source": [
    "### Available models in Pytorch\n",
    "\n",
    ">The torchvision.models subpackage contains definitions of models for addressing different tasks, including: image classification, pixelwise semantic segmentation, object detection, instance segmentation, person keypoint detection, video classification, and optical flow.\n",
    ">\n",
    "[Models in pytorch](https://pytorch.org/vision/0.16/models.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e00384-8558-486c-8fdf-3ab89e610760",
   "metadata": {},
   "source": [
    "### Use a VGG model in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eca864c1-bd00-4601-9ee2-9576d8732225",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg11-8a719046.pth\" to /root/.cache/torch/hub/checkpoints/vgg11-8a719046.pth\n",
      "100%|██████████| 507M/507M [00:41<00:00, 12.7MB/s] \n"
     ]
    }
   ],
   "source": [
    "model = models.vgg11(weights=VGG11_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90d0c53d-03a5-4774-b575-b69b6a4bdb21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU(inplace=True)\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aaf162-6778-41e7-9f11-625bd082e40f",
   "metadata": {},
   "source": [
    "[CIFAR10](https://pytorchaorg/tutorials/beginner/blitz/cifar10_tutorial.html) dataset <br>\n",
    "\n",
    ">It has the classes: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’. The images in CIFAR-10 are of size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d6ec4db4-07a5-4462-a081-a3eac57c8446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5))\n",
    "    # actual mean/std values are calculated from the actual CIFAR-10 dataset\n",
    "    transforms.Normalize(mean=(0.4914,0.4822,0.4465), std=(0.2023,0.1994,0.2010)) \n",
    "])\n",
    "classes = ('plane', 'car', 'bird', 'cat','deer', \n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "train_dataset=torchvision.datasets.CIFAR10(root=\"./data/cifar10_data\", train=True, download=True,transform=transform)\n",
    "val_dataset=torchvision.datasets.CIFAR10(root=\"./data/cifar10_data\", train=False, download=True,transform=transform)\n",
    "\n",
    "# train_dataset 50K images\n",
    "# use subset \n",
    "train_dataset_sub = Subset(train_dataset, range(1000))\n",
    "val_dataset_sub = Subset(val_dataset, range(500))\n",
    "\n",
    "batch_size=32\n",
    "train_loader = DataLoader(train_dataset_sub, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset_sub, batch_size=batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "209dd158-91d1-4a59-a146-16350ed98118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDiNHsJ7GBpGkcyuTncxP5VcuL77Kd1xNIN6jgueTV/Vp47VA4jQsW2qBwM1a0rwjH4guoJrlzFJOhaGF+gUYCn8TmvKcm3d9T00klZdDJjv9k5t51nSUjdtbI7defan3+pXlrHsjE0rYyNr9vxrtb/AOHEsiyJMj3FyUxEWfbg/wBa5Wa1kspWtbgbZYTsYHsRxQmtGN8uyZLaQ2tzqMX9oiQ2iS732KC3XoP89K7a+1ZreS71HT7WJUCxJA7Dd5arwRj/ADyawLjR5DdPNYFmjDElAD61UN5qkKypvmUMR8u0jkcVjTrKashuKubdp4tljvlu9UeSeJPmUhgu1/b0HasG6ln1fWnkhZDb3M2UV+4J5qlOiiIGeLeM/MhBIz649afFqLRWuqtZWUs1xBCixkqQihjg49wKc1KasZyjbY//2Q==",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im, label = train_dataset[0]\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "086170da-2023-486a-835a-e9164ce569d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 'frog')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label, classes[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f9f249d-14d1-4508-8966-453ad8fb7071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 32, 32])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = next(iter(train_loader))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "65407a94-f8de-4562-bebf-5c8bb6d746fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 5, 6, 0, 3, 1, 7, 2, 6, 7, 1, 4, 2, 2, 3, 4, 3, 9, 1, 7, 7, 0, 1, 5,\n",
       "        0, 0, 6, 3, 5, 5, 7, 1])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aedb56ec-d0b5-41c7-8432-99702da4179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(model.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f20d19a4-2f6e-455e-96a0-4757a51c9173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1000])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(X_train)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ebf3c776-4e53-4093-b588-023b3408c596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=4096, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfcd82b-d16e-4d65-82ea-5a52f0f79eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you use GPU\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d506889e-ff8d-4868-b637-d8853a301599",
   "metadata": {},
   "source": [
    "Currently, the number of output features is 1000. You need to change this classifier to 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5381669b-0afa-4166-8268-846ee90fc080",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier[-1] = nn.Linear(4096, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1be7e466-4bbc-4503-ad54-069cc3bd5b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(X_train)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f0a85c3d-f6a4-439d-838e-a926ead7191f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0: train error: 0.9726948123425245, validation error: 1.6468564346432686, validation accuracy: 0.5144531242549419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1: train error: 0.5757191590964794, validation error: 0.9803864434361458, validation accuracy: 0.6476562507450581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2: train error: 0.38066741777583957, validation error: 1.0013910122215748, validation accuracy: 0.6714843735098839\n",
      "Time to complete 34.54 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_losses, val_losses, val_accuracies = utils.learn(model, train_loader, val_loader, opt, F.cross_entropy, num_epoch=3)\n",
    "end = time.time()\n",
    "print(f'Time to complete {end - start:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054cfa28-6703-403e-9f6f-dcfa8b8aa8dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
